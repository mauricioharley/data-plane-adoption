[id="planning-the-new-deployment_{context}"]

//:context: planning

//kgilliga: This module will be converted to an assembly. Check xref contexts.

= Planning the new deployment

Just like you did back when you installed your Director deployed OpenStack, the
upgrade/migration to the podified OpenStack requires planning various aspects
of the environment such as node roles, planning your network topology, and
storage.

This document covers some of this planning, but it is recommended to read
the whole adoption guide before actually starting the process to be sure that
there is a global understanding of the whole process.

== Service configurations

There is a fundamental difference between the Director and Operator deployments
regarding the configuration of the services.

In Director deployments many of the service configurations are abstracted by
Director specific configuration options. A single Director option may trigger
changes for multiple services and support for drivers (for example Cinder's)
required patches to the Director code base.

In Operator deployments this approach has changed: reduce the installer specific knowledge and leverage OpenShift and
OpenStack service specific knowledge whenever possible.

To this effect OpenStack services will have sensible defaults for OpenShift
deployments and human operators will provide configuration snippets to provide
necessary configuration, such as cinder backend configuration, or to override
the defaults.

This shortens the distance between a service specific configuration file (such
as `cinder.conf`) and what the human operator provides in the manifests.

These configuration snippets are passed to the operators in the different
`customServiceConfig` sections available in the manifests, and then they are
layered in the services available in the following levels. To illustrate this,
if you were to set a configuration at the top Cinder level (`spec: cinder:
template:`) then it would be applied to all the cinder services; for example to
enable debug in all the cinder services you would do:

[source,yaml]
----
apiVersion: core.openstack.org/v1beta1
kind: OpenStackControlPlane
metadata:
  name: openstack
spec:
  cinder:
    template:
      customServiceConfig: |
        [DEFAULT]
        debug = True
< . . . >
----

If you only want to set it for one of the cinder services, for example the
scheduler, then you use the `cinderScheduler` section instead:

[source,yaml]
----
apiVersion: core.openstack.org/v1beta1
kind: OpenStackControlPlane
metadata:
  name: openstack
spec:
  cinder:
    template:
      cinderScheduler:
        customServiceConfig: |
          [DEFAULT]
          debug = True
< . . . >
----

In OpenShift it is not recommended to store sensitive information like the
credentials to the cinder storage array in the CRs, so most OpenStack operators
have a mechanism to use OpenShift's `Secrets` for sensitive configuration
parameters of the services and then use then by reference in the
`customServiceConfigSecrets` section which is analogous to the
`customServiceConfig`.

The contents of the `Secret` references passed in the
`customServiceConfigSecrets` will have the same format as `customServiceConfig`:
a snippet with the section/s and configuration options.

When there are sensitive information in the service configuration then it
becomes a matter of personal preference whether to store all the configuration
in the `Secret` or only the sensitive parts. However, if you split the
configuration between `Secret` and `customServiceConfig` you still need the
section header (eg: `[DEFAULT]`) to be present in both places.

Attention should be paid to each service's adoption process as they may have
some particularities regarding their configuration.

== Configuration tooling

In order to help users to handle the configuration for the TripleO and OpenStack
services the tool: https://github.com/openstack-k8s-operators/os-diff has been
develop to compare the configuration files between the TripleO deployment and
the next gen cloud.
Make sure Golang is installed and configured on your env:

----
git clone https://github.com/openstack-k8s-operators/os-diff
pushd os-diff
make build
----

Then configure ansible.cfg and ssh-config file according to your environment:

[source,yaml]
[subs=+quotes]
----
Host *
    IdentitiesOnly yes

Host virthost
    Hostname virthost
    IdentityFile ~/.ssh/id_rsa
    User root
    StrictHostKeyChecking no
    UserKnownHostsFile=/dev/null


Host standalone
    Hostname standalone
ifeval::["{build}" != "downstream"]
    IdentityFile ~/install_yamls/out/edpm/ansibleee-ssh-key-id_rsa
endif::[]
ifeval::["{build}" == "downstream"]
    IdentityFile *<path to SSH key>*
endif::[]
    User root
    StrictHostKeyChecking no
    UserKnownHostsFile=/dev/null

Host crc
    Hostname crc
    IdentityFile ~/.ssh/id_rsa
    User stack
    StrictHostKeyChecking no
    UserKnownHostsFile=/dev/null
----

And test your connection:

----
ssh -F ssh.config standalone
----

== Node roles

In Director deployments you had 4 different standard roles for the nodes:
`Controller`, `Compute`, `Ceph Storage`, `Swift Storage`, but in podified
OpenStack you make a distinction based on where things are running, in
OpenShift or external to it.

When adopting a Director OpenStack your `Compute` nodes will directly become
external nodes, so there should not be much additional planning needed there.

In many deployments being adopted the `Controller` nodes will require some
thought because you have many OpenShift nodes where the controller services
could run, and you have to decide which ones you want to use, how you are going to use them, and make sure those nodes are ready to run the services.

In most deployments running OpenStack services on `master` nodes can have a
seriously adverse impact on the OpenShift cluster, so it is recommended that you place OpenStack services on non `master` nodes.

By default OpenStack Operators deploy OpenStack services on any worker node, but
that is not necessarily what's best for all deployments, and there may be even
services that won't even work deployed like that.

When planing a deployment it's good to remember that not all the services on an
OpenStack deployments are the same as they have very different requirements.

Looking at the Cinder component you can clearly see different requirements for
its services: the cinder-scheduler is a very light service with low
memory, disk, network, and CPU usage; cinder-api service has a higher network
usage due to resource listing requests; the cinder-volume service will have a
high disk and network usage since many of its operations are in the data path
(offline volume migration, create volume from image, etc.), and then you have
the cinder-backup service which has high memory, network, and CPU (to compress
data) requirements.

The Glance and Swift components are in the data path, as well as RabbitMQ and Galera services.

Given these requirements it may be preferable not to let these services wander
all over your OpenShift worker nodes with the possibility of impacting other
workloads, or maybe you don't mind the light services wandering around but you
want to pin down the heavy ones to a set of infrastructure nodes.

There are also hardware restrictions to take into consideration, because if you
are using a Fibre Channel (FC) Cinder backend you need the cinder-volume,
cinder-backup, and maybe even the glance (if it's using Cinder as a backend)
services to run on a OpenShift host that has an HBA.

The OpenStack Operators allow a great deal of flexibility on where to run the
OpenStack services, as you can use node labels to define which OpenShift nodes
are eligible to run the different OpenStack services.  Refer to the xref:node-selector_{context}[About node
selector] to learn more about using labels to define
placement of the OpenStack services.

== TLS Everywhere

If the Director deployment was deployed with TLS Everywhere, FreeIPA (IDM) is used
for providing services with certificates. Certmonger is installed on all hosts,
which provisions the certificates for services running on the host.

The new Operator based deployment uses cert-manager to provision the certificates.
Instructions will be provided on how to retrieve the root CA from IPA and copy it
over to the new environment.

Because the same root CA is used to generate new certs, the currently used trust
chain doesn't have to be modified.

Note: The asumption is, that the new deployment will adopt the settings from the
old deployment, so in case TLS Everywhere is disabled, it won't be enabled on
the new deployment.

== Barbican

Barbican doesn't yet support all of the crypto plugins available in TripleO.

**TODO: Right now Barbican only supports the simple crypto plugin.

//*TODO: Talk about Ceph Storage and Swift Storage nodes, HCI deployments,
//etc.*

== Network

=== General information

With OpenShift, the network is a very important aspect of the deployment, and
it is important to plan it carefully. The general network requirements for the
OpenStack services are not much different from the ones in a Director
deployment, but the way you handle them is.

____
Note: More details about the network architecture and configuration can be
found in the
https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/18.0-dev-preview/html/deploying_red_hat_openstack_platform_18.0_development_preview_3_on_red_hat_openshift_container_platform/assembly_preparing-rhocp-for-rhosp#doc-wrapper[general
OpenStack documentation] as well as
https://docs.openshift.com/container-platform/4.14/networking/about-networking.html[OpenShift
Networking guide]. This document will address concerns specific to adoption.
____

// TODO: update the openstack link with the final documentation

// TODO: should we parametrize the version in the links somehow?

When adopting a new OpenStack deployment, it is important to align the network
configuration with the adopted cluster to maintain connectivity for existing
workloads.

The following logical configuration steps will incorporate the existing network
configuration:

- configure **OpenShift worker nodes** to align VLAN tags and IPAM
  configuration with the existing deployment.
- configure **podified OpenStack services** to use compatible IP ranges for
  service and load balancing IPs.
- configure **EDPM nodes** to use corresponding compatible configuration for
  VLAN tags and IPAM.

Specifically,

- **<<_ipam_planning,IPAM configuration>>** will either be reused from the
  **existing** deployment or, depending on IP address availability in the
  existing allocation pools, **new** ranges will be defined to be used for the
  new control plane services. If so, **IP routing** will be configured between
  the old and new ranges.
- **<<_vlan_tags,VLAN tags>>** will be reused from the existing deployment.

=== Pulling configuration from the existing deployment

Let's first determine which isolated networks are defined in the existing
deployment. You can find the network configuration in the `network_data.yaml`
file. For example,

```
- name: InternalApi
  mtu: 1500
  vip: true
  vlan: 20
  name_lower: internal_api
  dns_domain: internal.mydomain.tld.
  service_net_map_replace: internal
  subnets:
    internal_api_subnet:
      ip_subnet: '172.17.0.0/24'
      allocation_pools: [{'start': '172.17.0.4', 'end': '172.17.0.250'}]
```

You should make a note of the VLAN tag used (`vlan` key) and the IP range
(`ip_subnet` key) for each isolated network. The IP range will later be split
into separate pools for control plane services and load balancer IP addresses.

You should also determine the list of IP addresses already consumed in the
adopted environment. Consult `tripleo-ansible-inventory.yaml` file to find this
information. In the file, for each listed host, note IP and VIP addresses
consumed by the node.

For example,

```
Standalone:
  hosts:
    standalone:
      ...
      internal_api_ip: 172.17.0.100
    ...
  ...
standalone:
  children:
    Standalone: {}
  vars:
    ...
    internal_api_vip: 172.17.0.2
    ...
```

In the example above, note that the `172.17.0.2` and `172.17.0.100` are
consumed and won't be available for the new control plane services, at least
until the adoption is complete.

Repeat the process for each isolated network and each host in the
configuration.

---

At the end of this process, you should have the following information:

- A list of isolated networks used in the existing deployment.
- For each of the isolated networks, the VLAN tag and IP ranges used for
  dynamic address allocation.
- A list of existing IP address allocations used in the environment. You will
  later exclude these addresses from allocation pools available for the new
  control plane services.

=== IPAM planning

The new deployment model puts additional burden on the size of IP allocation
pools available for OpenStack services. This is because each service deployed
on OpenShift worker nodes will now require an IP address from the IPAM pool (in
the previous deployment model, all services hosted on a controller node shared
the same IP address.)

Since the new control plane deployment has different requirements as to the
number of IP addresses available for services, it may even be impossible to
reuse the existing IP ranges used in adopted environment, depending on its
size. Prudent planning is required to determine which options are available in
your particular case.

The total number of IP addresses required for the new control plane services,
in each isolated network, is calculated as a sum of the following:

- The number of OpenShift worker nodes. (Each node will require 1 IP address in
  `NodeNetworkConfigurationPolicy` CRs.)
- The number of IP addresses required for EDPM nodes. (Each node will require
  an IP address from `NetConfig` CRs.)
- The number of IP addresses required for control plane services. (Each service
  will require an IP address from `NetworkAttachmentDefinition` CRs.) This
  number depends on the number of replicas for each service.
- The number of IP addresses required for load balancer IP addresses. (Each
  service will require a VIP address from `IPAddressPool` CRs.)

As of the time of writing, the simplest single worker node OpenShift deployment
(CRC) has the following IP ranges defined (for the `internalapi` network):

- 1 IP address for the single worker node;
- 1 IP address for the EDPM node;
- `NetworkAttachmentDefinition` CRs for control plane services:
  `X.X.X.30-X.X.X.70` (41 addresses);
- `IPAllocationPool` CRs for load balancer IPs: `X.X.X.80-X.X.X.90` (11
  addresses).

Which comes to a total of 54 IP addresses allocated to the `internalapi`
allocation pools.

// TODO: update the numbers above for a more realistic multinode cluster.

The exact requirements may differ depending on the list of OpenStack services
to be deployed, their replica numbers, as well as the number of OpenShift
worker nodes and EDPM nodes.

Additional IP addresses may be required in future OpenStack releases, so it is
advised to plan for some extra capacity, for each of the allocation pools used
in the new environment.

Once you know the required IP pool size for the new deployment, you can choose
one of the following scenarios to handle IPAM allocation in the new
environment.

The first listed scenario is more general and implies using new IP ranges,
while the second scenario implies reusing the existing ranges. The end state of
the former scenario is using the new subnet ranges for control plane services,
but keeping the old ranges, with their node IP address allocations intact, for
EDP nodes.

==== Scenario 1: Use new subnet ranges

This scenario is compatible with any existing subnet configuration, and can be
used even when the existing cluster subnet ranges don't have enough free IP
addresses for the new control plane services.

The general idea here is to define new IP ranges for control plane services
that belong to a different subnet that was not used in the existing cluster.
Then, configure link local IP routing between the old and new subnets to allow
old and new service deployments to communicate. This involves using TripleO
mechanism on pre-adopted cluster to configure additional link local routes
there. This will allow EDP deployment to reach out to adopted nodes using their
old subnet addresses.

The new subnet should be sized appropriately to accommodate the new control
plane services, but otherwise doesn't have any specific requirements as to the
existing deployment allocation pools already consumed. Actually, the
requirements as to the size of the new subnet are lower than in the second
scenario, as the old subnet ranges are kept for the adopted nodes, which means
they don't consume any IP addresses from the new range.

In this scenario, you will configure `NetworkAttachmentDefinition` CRs to use a
different subnet from what will be configured in `NetConfig` CR for the same
networks. The former range will be used for podified control plane services,
while the latter will be used to manage IPAM for EDP nodes.

During the process, you will need to make sure that adopted node IP addresses
don't change during the adoption process. This is achieved by listing the
addresses in `fixedIP` fields in `OpenstackDataplaneNodeSet` per-node section.

---

Before proceeding, configure host routes on the adopted nodes for the podified
control plane subnets.

To achieve this, you will need to re-run `tripleo deploy` with additional
`routes` entries added to `network_config`. (This change should be applied
for every adopted node configuration.) For example, you may add the following
to `net_config.yaml`:

```yaml
network_config:
  - type: ovs_bridge
    name: br-ctlplane
    routes:
    - ip_netmask: 0.0.0.0/0
      next_hop: 192.168.1.1
    - ip_netmask: 172.31.0.0/24  # <- new ctlplane subnet
      next_hop: 192.168.1.100    # <- adopted node ctlplane IP address
```

Do the same for other networks that will need to use different subnets for the
new and old parts of the deployment.

Once done, run `tripleo deploy` to apply the new configuration.

Note that network configuration changes are not applied by default to avoid
risk of network disruption. You will have to enforce the changes by setting the
`StandaloneNetworkConfigUpdate: true` in the TripleO configuration files.

Once `tripleo deploy` is complete, you should see new link local routes to the
new subnet on each node. For example,

```bash
# ip route | grep 172
172.31.0.0/24 via 192.168.122.100 dev br-ctlplane
```

---

The next step is to configure similar routes for the old subnet for podified
services attached to the networks. This is done by adding `routes` entries to
`NodeNetworkConfigurationPolicy` CRs for each network. For example,

```yaml
      - destination: 192.168.122.0/24
        next-hop-interface: ospbr
```

Once applied, you should eventually see the following route added to your OCP nodes.

```bash
# ip route | grep 192
192.168.122.0/24 dev ospbr proto static scope link
```

---

At this point, you should be able to ping the adopted nodes from OCP nodes
using their old subnet addresses; and vice versa.

---


Finally, during EDPM adoption, you will have to take care of several aspects:

- in network_config, add link local routes to the new subnets, for example:

```yaml
  nodeTemplate:
    ansible:
      ansibleUser: root
      ansibleVars:
        additional_ctlplane_host_routes:
        - ip_netmask: 172.31.0.0/24
          next_hop: '{{ ctlplane_ip }}'
        edpm_network_config_template: |
          network_config:
          - type: ovs_bridge
            routes: {{ ctlplane_host_routes + additional_ctlplane_host_routes }}
            ...
```

- list the old IP addresses as `ansibleHost` and `fixedIP`, for example:

```yaml
  nodes:
    standalone:
      ansible:
        ansibleHost: 192.168.122.100
        ansibleUser: ""
      hostName: standalone
      networks:
      - defaultRoute: true
        fixedIP: 192.168.122.100
        name: ctlplane
        subnetName: subnet1
```

- expand SSH range for the firewall configuration to include both subnets:

```yaml
        edpm_sshd_allowed_ranges:
        - 192.168.122.0/24
        - 172.31.0.0/24
```

This is to allow SSH access from the new subnet to the adopted nodes as well as
the old one.

---

Since you are applying new network configuration to the nodes, consider also
setting `edpm_network_config_update: true` to enforce the changes.

---

Note that the examples above are incomplete and should be incorporated into
your general configuration.

==== Scenario 2: Reuse existing subnet ranges

This scenario is only applicable when the existing subnet ranges have enough IP
addresses for the new control plane services. On the other hand, it allows to
avoid additional routing configuration between the old and new subnets, as in
<<_scenario_1_use_new_subnet_ranges,scenario 1>>.

The general idea here is to instruct the new control plane services to use the
same subnet as in the adopted environment, but define allocation pools used by
the new services in a way that would exclude IP addresses that were already
allocated to existing cluster nodes.

This scenario implies that the remaining IP addresses in the existing subnet is
enough for the new control plane services. If not,
<<_scenario_1_use_new_subnet_ranges,the first scenario>> should be used
instead. Please consult <<_ipam_planning,IPAM planning>> for more details.

No special routing configuration is required in this scenario; the only thing
to pay attention to is to make sure that already consumed IP addresses don't
overlap with the new allocation pools configured for OpenStack podified control
services.

If you are especially constrained by the size of the existing subnet, you may
have to apply elaborate exclusion rules when defining allocation pools for the
new control plane services. You can find details on how to do this in the
appropriate sections below.

=== VLAN tags

Regardless of the IPAM scenario, the VLAN tags used in the existing deployment
will be reused in the new deployment. Depending on the scenario, the IP address
ranges to be used for control plane services will be either reused from the old
deployment or defined anew. Adjust the configuration described below
accordingly.

=== Configuration steps

At this point, you should have a good idea about VLAN and IPAM configuration
you would like to replicate in the new environment.

Before proceeding, you should have a list of the following IP address
allocations to be used for the new podified control plane services:

- 1 IP address, per isolated network, per OpenShift worker node. (These
  addresses will <<_configure_openshift_worker_nodes,translate>> to
  `NodeNetworkConfigurationPolicy` CRs.)
- IP range, per isolated network, for EDPM nodes. (These ranges will
  <<_configure_edpm_nodes,translate>> to `NetConfig` CRs.)
- IP range, per isolated network, for control plane services. (These ranges
  will <<_pod_connectivity_to_isolated_networks,translate>> to
  `NetworkAttachmentDefinition` CRs.)
- IP range, per isolated network, for load balancer IP addresses. (These ranges
  will <<_load_balancer_ip_addresses,translate>> to `IPAddressPool` CRs for
  MetalLB.)

____
Make sure you have the information listed above before proceeding with the next
steps!
____

____
Note: The exact list and configuration of isolated networks in the examples
listed below should reflect the actual adopted environment. The number of
isolated networks may differ from the example below. IPAM scheme may differ.
Only relevant parts of the configuration are shown. Examples are incomplete and
should be incorporated into the general configuration for the new deployment,
as described in the general OpenStack documentation.
____

=== Configure OpenShift worker nodes

OCP worker nodes that run OpenStack services need a way to connect the service
pods to isolated networks. This requires physical network configuration on the
hypervisor.

This configuration is managed by the NMState operator, which uses the CRs to
define the desired network configuration for the nodes.

For each node, define a `NodeNetworkConfigurationPolicy` CR that describes the
desired network configuration. See the example below.

```
apiVersion: v1
items:
- apiVersion: nmstate.io/v1
  kind: NodeNetworkConfigurationPolicy
  spec:
      interfaces:
      - description: internalapi vlan interface
        ipv4:
          address:
          - ip: 172.17.0.10
            prefix-length: 24
          dhcp: false
          enabled: true
        ipv6:
          enabled: false
        name: enp6s0.20
        state: up
        type: vlan
        vlan:
          base-iface: enp6s0
          id: 20
          reorder-headers: true
      - description: storage vlan interface
        ipv4:
          address:
          - ip: 172.18.0.10
            prefix-length: 24
          dhcp: false
          enabled: true
        ipv6:
          enabled: false
        name: enp6s0.21
        state: up
        type: vlan
        vlan:
          base-iface: enp6s0
          id: 21
          reorder-headers: true
      - description: tenant vlan interface
        ipv4:
          address:
          - ip: 172.19.0.10
            prefix-length: 24
          dhcp: false
          enabled: true
        ipv6:
          enabled: false
        name: enp6s0.22
        state: up
        type: vlan
        vlan:
          base-iface: enp6s0
          id: 22
          reorder-headers: true
    nodeSelector:
      kubernetes.io/hostname: ocp-worker-0
      node-role.kubernetes.io/worker: ""
```

=== Configure podified OpenStack services

==== Pod connectivity to isolated networks

Once NMState operator created the desired hypervisor network configuration for
isolated networks, we need to configure OpenStack services to use configured
interfaces. This is achieved by defining `NetworkAttachmentDefinition` CRs for
each isolated network. (In some clusters, these CRs are managed by
https://docs.openshift.com/container-platform/4.14/networking/cluster-network-operator.html[Cluster
Network Operator], in which case `Network` CRs should be used instead.)

For example,

```
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
spec:
  config: |
    {
      "cniVersion": "0.3.1",
      "name": "internalapi",
      "type": "macvlan",
      "master": "enp6s0.20",
      "ipam": {
        "type": "whereabouts",
        "range": "172.17.0.0/24",
        "range_start": "172.17.0.20",
        "range_end": "172.17.0.50"
      }
    }
```

Make sure that the interface name and IPAM range match the configuration used
in `NodeNetworkConfigurationPolicy` CRs.

When reusing existing IP ranges, you may exclude part of the range defined by
`range_start` and `range_end` that was already consumed in the existing
deployment. Please use `exclude` as follows.

```
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
spec:
  config: |
    {
      "cniVersion": "0.3.1",
      "name": "internalapi",
      "type": "macvlan",
      "master": "enp6s0.20",
      "ipam": {
        "type": "whereabouts",
        "range": "172.17.0.0/24",
        "range_start": "172.17.0.20",
        "range_end": "172.17.0.50",
        "exclude": [
          "172.17.0.24/32",
          "172.17.0.44/31"
        ]
      }
    }
```

The example above would exclude addresses `172.17.0.24` as well as
`172.17.0.44` and `172.17.0.45` from the allocation pool.

==== Load balancer IP addresses

Some OpenStack services require load balancer IP addresses. These IP addresses
belong to the same IP range as the control plane services, and are managed by
MetalLB. The IP address pool is defined by `IPAllocationPool` CRs. This pool
should also be aligned with the adopted configuration.

For example,

```
- apiVersion: metallb.io/v1beta1
  kind: IPAddressPool
  spec:
    addresses:
    - 172.17.0.60-172.17.0.70
```

Define `IPAddressPool` CRs for each isolated network that requires load
balancer IP addresses.

When reusing existing IP ranges, you may exclude part of the range by listing
multiple `addresses` entries.

For example,

```
- apiVersion: metallb.io/v1beta1
  kind: IPAddressPool
  spec:
    addresses:
    - 172.17.0.60-172.17.0.64
    - 172.17.0.66-172.17.0.70
```

The example above would exclude the `172.17.0.65` address from the allocation
pool.

// TODO: is there anything specific to mention about BGP L3 mode here?

=== Configure EDPM nodes

A complete OpenStack cluster consists of OpenShift nodes and EDPM nodes. The
former use `NodeNetworkConfigurationPolicy` CRs to configure physical
interfaces. Since EDPM nodes are not OpenShift nodes, a different approach to
configure their network connectivity is used.

Instead, EDPM nodes are configured by `dataplane-operator` and its CRs. The CRs
define desired network configuration for the nodes.

In case of adoption, the configuration should reflect the existing network
setup. You should be able to pull `net_config.yaml` files from each node and
reuse it when defining `OpenstackDataplaneNodeSet`. The format of the
configuration hasn't changed (`os-net-config` is still being used under the
hood), so you should be able to put network templates under
`edpm_network_config_template` variables (either common for all nodes, or
per-node).

To make sure the latest network configuration is used during EDPM adoption, you
should also set `edpm_network_config_update: true` in the `nodeTemplate`.

You will proceed with <<adopting-edpm_openstack-adoption,EDPM adoption
process>> once the OpenStack podified control plane is deployed in the
OpenShift cluster. When doing so, you will configure `NetConfig` and
`OpenstackDataplaneNodeSet` CRs, using the same VLAN tags and IPAM
configuration as determined in the previous steps.

For example,

```
apiVersion: network.openstack.org/v1beta1
kind: NetConfig
metadata:
  name: netconfig
spec:
  networks:
  - name: internalapi
    dnsDomain: internalapi.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.17.0.250
        start: 172.17.0.100
      cidr: 172.17.0.0/24
      vlan: 20
  - name: storage
    dnsDomain: storage.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.18.0.250
        start: 172.18.0.100
      cidr: 172.18.0.0/24
      vlan: 21
  - name: tenant
    dnsDomain: tenant.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.19.0.250
        start: 172.19.0.100
      cidr: 172.19.0.0/24
      vlan: 22
```

List multiple `allocationRanges` entries to exclude some of the IP addresses,
e.g. to accommodate for addresses already consumed by the adopted environment.

```
apiVersion: network.openstack.org/v1beta1
kind: NetConfig
metadata:
  name: netconfig
spec:
  networks:
  - name: internalapi
    dnsDomain: internalapi.example.com
    subnets:
    - name: subnet1
      allocationRanges:
      - end: 172.17.0.199
        start: 172.17.0.100
      - end: 172.17.0.250
        start: 172.17.0.201
      cidr: 172.17.0.0/24
      vlan: 20
```

The example above would exclude the `172.17.0.200` address from the pool.

== Storage

When looking into the storage in an OpenStack deployment you can differentiate
2 different kinds, the storage requirements of the services themselves and the
storage used for the OpenStack users that the services will manage.

These requirements may drive your OpenShift node selection, as mentioned above,
and may require you to do some preparations on the OpenShift nodes before
you can deploy the services.

//*TODO: Galera, RabbitMQ, Swift, Glance, etc.*

=== Cinder requirements

The Cinder service has both local storage used by the service and OpenStack user
requirements.

Local storage is used for example when downloading a glance image for the create
volume from image operation, which can become considerable when having
concurrent operations and not using cinder volume cache.

In the Operator deployed OpenStack, there is a way to configure the
location of the conversion directory to be an NFS share (using the extra
volumes feature), something that needed to be done manually before.

Even if it's an adoption and it may seem that there's nothing to consider
regarding the Cinder backends, because you are using the same ones that you are
using in your current deployment, you should still evaluate it, because it may not be so straightforward.

First you need to check the transport protocol the Cinder backends are using:
RBD, iSCSI, FC, NFS, NVMe-oF, etc.

Once you know all the transport protocols that you are using, you can make
sure that you are taking them into consideration when placing the Cinder services
(as mentioned above in the Node Roles section) and the right storage transport
related binaries are running on the OpenShift nodes.

Detailed information about the specifics for each storage transport protocol can
be found in the xref:adopting-the-block-storage-service_{context}[Adopting the Block Storage service].
